{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lexis_functions as lf\n",
    "import datetime as dt\n",
    "\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in / Clean Data\n",
    "\n",
    "- 5,391 participants\n",
    "- 4,995 (92%) report no jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\jc4673\\Documents\\CHS_Lexis\\LexisNexis\\LN_NOID_DATA\\original\\\\\n",
    "LN_Output_Employment_LN_InputLexisNexisCHSParticipantsNS.Dataset.csv\",\n",
    "                index_col='ssn_altkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column naming convention to match the other files, with chronological number at the end\n",
    "cols = df.columns.tolist()\n",
    "for i, colname in enumerate(cols):\n",
    "    if 'pawk' in colname:\n",
    "        shift = colname[:4] + colname[6:] + colname[4:6]\n",
    "        cols[i] = shift\n",
    "\n",
    "df_time = df.copy()\n",
    "df_time.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_time.iloc[:, 4:].dropna(how='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participants\n",
    "len(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of participants with no job\n",
    "len(df_time) - len(df_time.iloc[:, 4:].dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervals of Employment\n",
    "lex_employmentstatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = ['yrdeath'] + [col for col in df_time.columns if '_seen' in col]\n",
    "df_time = df_time[my_cols].reset_index(drop=False)\n",
    "#Duplicates present for some reason\n",
    "df_time.drop_duplicates(inplace=True, subset='ssn_altkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.wide_to_long(df_time, ['pawk_last_seen_', 'pawk_first_seen_'], i='ssn_altkey', j='num')\n",
    "df_long = df_long.sort_index().dropna(subset=['pawk_last_seen_', 'pawk_first_seen_'])\n",
    "df_long.columns = ['death', 'last_seen_date', 'first_seen_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning - convert dates, drop duplicates, and drop records where last_seen_date == first_seen_date\n",
    "df_long = lf.convert_all_dates(df_long).drop_duplicates()\n",
    "df_long = df_long[df_long['last_seen_date'] - df_long['first_seen_date'] != dt.timedelta(days=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductionFunction(data):\n",
    "    \"\"\"A function \"\"\"\n",
    "    #data.reset_index(drop=False, inplace=True)\n",
    "    # create a 2D graph of connectivity between date ranges\n",
    "    start = data.first_seen_date.values\n",
    "    end = data.last_seen_date.values\n",
    "    graph = (start <= end[:, None]) & (end >= start[:, None])\n",
    "\n",
    "    # find connected components in this graph\n",
    "    n_components, indices = connected_components(graph)\n",
    "\n",
    "    # group the results by these connected components\n",
    "    return data.groupby(indices).aggregate({'first_seen_date': 'min',\n",
    "                                            'last_seen_date': 'max',\n",
    "                                            'num': 'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_reduced = df_long.reset_index(drop=False).groupby('ssn_altkey').apply(lambda x :reductionFunction(x))\n",
    "del df_long_reduced['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of participants that have dates connected to them\n",
    "len(set(df_long_reduced.index.get_level_values(0).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Consistency\n",
    "\n",
    "93 of the 396 persons who report jobs (46%) report dates for those jobs.  This corresponds to 1.7% of total participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_reduced.to_csv(r\"C:\\Users\\jc4673\\Documents\\CHS_Lexis\\LexisNexis\\LN_NOID_DATA\\derived\\employment_range_derived.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Number of Jobs\n",
    "lex_numberofjobs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = cols\n",
    "g1 = df[[x for x in df.columns.tolist() if '1' in x]]\n",
    "g2 = df[[x for x in df.columns.tolist() if '2' in x]]\n",
    "g3 = df[[x for x in df.columns.tolist() if '3' in x]]\n",
    "g4 = df[[x for x in df.columns.tolist() if '4' in x]]\n",
    "g5 = df[[x for x in df.columns.tolist() if '5' in x]]\n",
    "\n",
    "all_subs = [g1, g2, g3, g4, g5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find whether any values exist for the 1st-5th job\n",
    "final = pd.DataFrame()\n",
    "for sub in all_subs:\n",
    "    a = sub.apply(lambda x: x.any(), axis=1)\n",
    "    a[a == False] = 0\n",
    "    a[a != 0] = 1\n",
    "    final = pd.concat([final, a.to_frame()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sum = final.apply(sum, axis=1)\n",
    "final_sum.rename('lex_numberofjobs_c', inplace=True)\n",
    "final_sum = final_sum.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sum.to_csv(r\"C:\\Users\\jc4673\\Documents\\LexisNexis\\LN_NOID_DATA\\derived\\number_of_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py365]",
   "language": "python",
   "name": "conda-env-py365-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
